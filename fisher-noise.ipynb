{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89a4387c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-14T07:41:56.440612Z",
     "iopub.status.busy": "2023-11-14T07:41:56.440330Z",
     "iopub.status.idle": "2023-11-14T07:42:00.670325Z",
     "shell.execute_reply": "2023-11-14T07:42:00.669348Z"
    },
    "papermill": {
     "duration": 4.238598,
     "end_time": "2023-11-14T07:42:00.672986",
     "exception": false,
     "start_time": "2023-11-14T07:41:56.434388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35f6ac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T07:42:00.686539Z",
     "iopub.status.busy": "2023-11-14T07:42:00.685707Z",
     "iopub.status.idle": "2023-11-14T07:42:00.754807Z",
     "shell.execute_reply": "2023-11-14T07:42:00.753762Z"
    },
    "papermill": {
     "duration": 0.07903,
     "end_time": "2023-11-14T07:42:00.757003",
     "exception": false,
     "start_time": "2023-11-14T07:42:00.677973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def parameter_count(model):\n",
    "    total_count = 0\n",
    "    trainable_count = 0\n",
    "    for p in model.parameters():\n",
    "        total_count += torch.prod(torch.tensor(p.shape)).item()\n",
    "        if p.requires_grad:\n",
    "            trainable_count += torch.prod(torch.tensor(p.shape)).item()\n",
    "\n",
    "    return total_count, trainable_count\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device = {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c672034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T07:42:00.767275Z",
     "iopub.status.busy": "2023-11-14T07:42:00.767004Z",
     "iopub.status.idle": "2023-11-14T07:42:00.773186Z",
     "shell.execute_reply": "2023-11-14T07:42:00.772151Z"
    },
    "papermill": {
     "duration": 0.013213,
     "end_time": "2023-11-14T07:42:00.774992",
     "exception": false,
     "start_time": "2023-11-14T07:42:00.761779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MovingAverage:\n",
    "    def __init__(self, name, rd=4):\n",
    "        self.name = name\n",
    "        # avg value\n",
    "        self.val = 0.0\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "        self.rd = rd\n",
    "\n",
    "    def update(self, x):\n",
    "        self.sum += x\n",
    "        self.count += 1\n",
    "\n",
    "        # update self.value\n",
    "        self.val = round(self.sum / self.count, self.rd)\n",
    "\n",
    "    def value(self) -> float:\n",
    "        return self.val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0cd454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T07:42:00.784755Z",
     "iopub.status.busy": "2023-11-14T07:42:00.784315Z",
     "iopub.status.idle": "2023-11-14T07:42:00.791255Z",
     "shell.execute_reply": "2023-11-14T07:42:00.790398Z"
    },
    "papermill": {
     "duration": 0.013754,
     "end_time": "2023-11-14T07:42:00.793071",
     "exception": false,
     "start_time": "2023-11-14T07:42:00.779317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HiddenDataset(Dataset):\n",
    "    def __init__(self, df, base_dir):\n",
    "        super().__init__()\n",
    "        df['image_path'] = df['image_id'].apply(lambda x: os.path.join(base_dir,'images', x.split('-')[0], x.split('-')[1] + '.png'))\n",
    "        self.df = df\n",
    "\n",
    "        # read the images at the init only\n",
    "        # self.images = [\n",
    "        #    torch.tensor(np.transpose(np.array(Image.open(x).convert('RGB')), [2, 0, 1])) for x in self.df['image_path'].tolist()\n",
    "        # ]\n",
    "        self.images = [torchvision.io.read_image(x) for x in self.df['image_path'].tolist()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        age = self.df['age_group'].iloc[index]\n",
    "        return image, age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1653bc80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T07:42:00.802756Z",
     "iopub.status.busy": "2023-11-14T07:42:00.802319Z",
     "iopub.status.idle": "2023-11-14T07:42:00.811872Z",
     "shell.execute_reply": "2023-11-14T07:42:00.811057Z"
    },
    "papermill": {
     "duration": 0.016379,
     "end_time": "2023-11-14T07:42:00.813723",
     "exception": false,
     "start_time": "2023-11-14T07:42:00.797344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hessian(dataset, model_init):\n",
    "    T1 = time.time()\n",
    "    model_init.eval()\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model_init.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "\n",
    "    count = 0\n",
    "    for X, y in dataloader:\n",
    "        X = X.float().to(device)\n",
    "        y = y.long().to(device)\n",
    "        output = model_init(X)\n",
    "        probs = torch.softmax(output, dim=1).data\n",
    "\n",
    "        for classidx in range(output.shape[1]):\n",
    "            target = torch.empty_like(y).fill_(classidx)\n",
    "            loss = loss_fn(output, target)\n",
    "            model_init.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model_init.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (y == target).float() * p.grad.data\n",
    "                    p.grad2_acc += probs[:, classidx] * torch.pow(p.grad.data, 2)\n",
    "\n",
    "        count += 1\n",
    "        print(f\"{count}/{len(dataset)}\", end=\"\\r\")\n",
    "\n",
    "    for p in model_init.parameters():\n",
    "        p.grad_acc /= len(dataset)\n",
    "        p.grad2_acc /= len(dataset)\n",
    "\n",
    "    T2 = time.time()\n",
    "    timetaken = round(T2 - T1, 3)\n",
    "    print(f\"Time taken = {timetaken} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "068bed83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T07:42:00.823265Z",
     "iopub.status.busy": "2023-11-14T07:42:00.823027Z",
     "iopub.status.idle": "2023-11-14T07:42:00.828562Z",
     "shell.execute_reply": "2023-11-14T07:42:00.827870Z"
    },
    "papermill": {
     "duration": 0.012372,
     "end_time": "2023-11-14T07:42:00.830336",
     "exception": false,
     "start_time": "2023-11-14T07:42:00.817964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n",
    "    # save the file while saving the version\n",
    "    # subprocess.run('touch submission.zip', shell=True)\n",
    "    base_dir = \"/kaggle/input/mock-cifar10-data\"\n",
    "    num_checkpoints = 10\n",
    "    real_run = False\n",
    "else:\n",
    "    # this part will run when we submit to kaggle.\n",
    "    base_dir = \"/kaggle/input/neurips-2023-machine-unlearning/\"\n",
    "    num_checkpoints = 512\n",
    "    real_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f456db2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T07:42:00.839752Z",
     "iopub.status.busy": "2023-11-14T07:42:00.839519Z",
     "iopub.status.idle": "2023-11-14T07:46:44.590880Z",
     "shell.execute_reply": "2023-11-14T07:46:44.589959Z"
    },
    "papermill": {
     "duration": 283.762841,
     "end_time": "2023-11-14T07:46:44.597455",
     "exception": false,
     "start_time": "2023-11-14T07:42:00.834614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the model\n",
      "Loading the model from checkpoint = /kaggle/input/mock-cifar10-data/original_model.pth\n",
      "Initializing the retain dataset\n",
      "Initializing the forget dataset\n",
      "Initializing the validation dataset\n",
      "length of retain dataset = 27440\n",
      "length of forget dataset = 560\n",
      "length of validation dataset = 3500\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('/kaggle/tmp', exist_ok=True)\n",
    "\n",
    "print(f\"Initializing the model\")\n",
    "model = resnet18(weights=None, num_classes=10)\n",
    "original_path = os.path.join(base_dir, 'original_model.pth')\n",
    "print(f\"Loading the model from checkpoint = {original_path}\")\n",
    "model.load_state_dict(torch.load(original_path))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "retain_df = pd.read_csv(os.path.join(base_dir, \"retain.csv\"))\n",
    "forget_df = pd.read_csv(os.path.join(base_dir, \"forget.csv\"))\n",
    "validation_df = pd.read_csv(os.path.join(base_dir, \"validation.csv\"))\n",
    "\n",
    "print(f\"Initializing the retain dataset\")\n",
    "retain_dataset = HiddenDataset(retain_df, base_dir)\n",
    "\n",
    "print(f\"Initializing the forget dataset\")\n",
    "forget_dataset = HiddenDataset(forget_df, base_dir)\n",
    "\n",
    "print(f\"Initializing the validation dataset\")\n",
    "validation_dataset = HiddenDataset(validation_df, base_dir)\n",
    "\n",
    "print(f\"length of retain dataset = {len(retain_dataset)}\")\n",
    "print(f\"length of forget dataset = {len(forget_dataset)}\")\n",
    "print(f\"length of validation dataset = {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce7fe4e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T07:46:44.608203Z",
     "iopub.status.busy": "2023-11-14T07:46:44.607893Z",
     "iopub.status.idle": "2023-11-14T08:28:34.478888Z",
     "shell.execute_reply": "2023-11-14T08:28:34.478031Z"
    },
    "papermill": {
     "duration": 2509.878521,
     "end_time": "2023-11-14T08:28:34.480759",
     "exception": false,
     "start_time": "2023-11-14T07:46:44.602238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken = 2509.847 seconds\n"
     ]
    }
   ],
   "source": [
    "model_scrub = copy.deepcopy(model)\n",
    "\n",
    "hessian(retain_dataset, model_scrub)\n",
    "# hessian(forget_dataset, model_scrub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c37274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T08:28:35.851280Z",
     "iopub.status.busy": "2023-11-14T08:28:35.850570Z",
     "iopub.status.idle": "2023-11-14T08:28:35.858822Z",
     "shell.execute_reply": "2023-11-14T08:28:35.857951Z"
    },
    "papermill": {
     "duration": 0.695967,
     "end_time": "2023-11-14T08:28:35.860674",
     "exception": false,
     "start_time": "2023-11-14T08:28:35.164707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_variance(grad2, alpha):\n",
    "    # variance as sqrt(1 / H).\n",
    "    # actually in the paper authors mentioned that -1/2 is better that -1/4 (mentioned in appendix section)\n",
    "    # we can try both and see which one works better.\n",
    "    var = copy.deepcopy(torch.pow(grad2 + 1e-8, -0.25))\n",
    "    var = var.clamp(max=1e3)\n",
    "\n",
    "    if grad2.size(0) == 10:\n",
    "        var = var.clamp(max=1e2)\n",
    "\n",
    "    var = alpha * var\n",
    "\n",
    "    if grad2.ndim > 1:\n",
    "        # here it means we have parameters as filters.\n",
    "        # in that case take the mean over all the filters.\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(grad2).clone()\n",
    "\n",
    "    if grad2.size(0) == 10:\n",
    "        # last layer\n",
    "        var *= 10\n",
    "    elif grad2.ndim == 1:\n",
    "        # batch norm layers\n",
    "        var *= 10\n",
    "\n",
    "    return var\n",
    "\n",
    "\n",
    "def get_newton_step(grad, grad2, beta):\n",
    "    # grad / H is the newton step\n",
    "    delta = copy.deepcopy(grad * torch.pow(grad2 + 1e-8, -1.0))\n",
    "    # delta = delta.clamp(max=1e2)\n",
    "    delta = beta * delta\n",
    "    return delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d648f567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T08:28:37.285593Z",
     "iopub.status.busy": "2023-11-14T08:28:37.285246Z",
     "iopub.status.idle": "2023-11-14T08:28:37.297251Z",
     "shell.execute_reply": "2023-11-14T08:28:37.296381Z"
    },
    "papermill": {
     "duration": 0.755474,
     "end_time": "2023-11-14T08:28:37.299272",
     "exception": false,
     "start_time": "2023-11-14T08:28:36.543798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(model_init, dataloader):\n",
    "    model_init.eval()\n",
    "    gt = np.array([])\n",
    "    pred = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.float().to(device)\n",
    "            y = y.long().to(device)\n",
    "\n",
    "            out = model_init(X)\n",
    "            y_pred = torch.argmax(out, dim=1)\n",
    "\n",
    "            gt = np.append(gt, y.cpu().numpy())\n",
    "            pred = np.append(pred, y_pred.cpu().numpy())\n",
    "\n",
    "    acc = round(float(np.mean(gt == pred)), 6)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def unlearning(\n",
    "    model,\n",
    "    model_scrub,\n",
    "    retain_loader,\n",
    "    forget_loader,\n",
    "    validation_loader,\n",
    "    device\n",
    "):\n",
    "    alpha = 5e-7\n",
    "    beta = 0.0\n",
    "\n",
    "    # evaluate first\n",
    "    retain_acc = calculate_accuracy(model_scrub, retain_loader)\n",
    "    forget_acc = calculate_accuracy(model_scrub, forget_loader)\n",
    "    validation_acc = calculate_accuracy(model_scrub, validation_loader)\n",
    "\n",
    "    print(f\"Initial retain acc = {retain_acc}, forget acc = {forget_acc}, validation acc = {validation_acc}\")\n",
    "\n",
    "    # first create a copy of the current scrub model\n",
    "    model_scrub_updated = copy.deepcopy(model_scrub)\n",
    "    # name to param mapping\n",
    "    scrub_mapping = {name : param for name, param in model_scrub.named_parameters()}\n",
    "\n",
    "    # add noise to model scrub updated using grad^2\n",
    "    for name, param in model_scrub_updated.named_parameters():\n",
    "        # in paper authors mentioned that newton update didn't work if hessian is only diagnoal \n",
    "        # and only adding noise worked better. but here I am trying the full update.\n",
    "        # if we don't want to use newton update step, make beta = 0.0\n",
    "        delta = get_newton_step(scrub_mapping[name].grad_acc, scrub_mapping[name].grad2_acc, beta=beta)\n",
    "        variance = get_variance(scrub_mapping[name].grad2_acc, alpha=alpha)\n",
    "        # here i am doing one step update. that is w - beta * (G * H^(-1)) + N(0, alpha * H^(-0.25))\n",
    "        param.data = param.data - delta + torch.sqrt(variance) * torch.randn_like(variance)\n",
    "\n",
    "    # evaluate now\n",
    "    retain_acc_update = calculate_accuracy(model_scrub_updated, retain_loader)\n",
    "    forget_acc_update = calculate_accuracy(model_scrub_updated, forget_loader)\n",
    "    validation_acc_update = calculate_accuracy(model_scrub_updated, validation_loader)\n",
    "\n",
    "    print(f\"After scrub retain acc = {retain_acc_update}, forget acc = {forget_acc_update}, validation acc = {validation_acc_update}\")\n",
    "\n",
    "    return model_scrub_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "582962f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T08:28:38.778047Z",
     "iopub.status.busy": "2023-11-14T08:28:38.777683Z",
     "iopub.status.idle": "2023-11-14T08:29:46.382872Z",
     "shell.execute_reply": "2023-11-14T08:29:46.381830Z"
    },
    "papermill": {
     "duration": 68.357062,
     "end_time": "2023-11-14T08:29:46.384906",
     "exception": false,
     "start_time": "2023-11-14T08:28:38.027844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for checkpoint = 0\n",
      "Initial retain acc = 0.979701, forget acc = 0.985714, validation acc = 0.725714\n",
      "After scrub retain acc = 0.979774, forget acc = 0.985714, validation acc = 0.727429\n",
      "Running for checkpoint = 1\n",
      "Initial retain acc = 0.979701, forget acc = 0.985714, validation acc = 0.725714\n",
      "After scrub retain acc = 0.979774, forget acc = 0.980357, validation acc = 0.726571\n",
      "Running for checkpoint = 2\n",
      "Initial retain acc = 0.979701, forget acc = 0.985714, validation acc = 0.725714\n",
      "After scrub retain acc = 0.978171, forget acc = 0.983929, validation acc = 0.724286\n",
      "Running for checkpoint = 3\n",
      "Initial retain acc = 0.979701, forget acc = 0.985714, validation acc = 0.725714\n",
      "After scrub retain acc = 0.978243, forget acc = 0.980357, validation acc = 0.725143\n",
      "Running for checkpoint = 4\n",
      "Initial retain acc = 0.979701, forget acc = 0.985714, validation acc = 0.725714\n",
      "After scrub retain acc = 0.980211, forget acc = 0.983929, validation acc = 0.729143\n",
      "Running for checkpoint = 5\n",
      "Initial retain acc = 0.979701, forget acc = 0.985714, validation acc = 0.725714\n",
      "After scrub retain acc = 0.97828, forget acc = 0.983929, validation acc = 0.726857\n",
      "Running for checkpoint = 6\n",
      "Initial retain acc = 0.979701, forget acc = 0.985714, validation acc = 0.725714\n",
      "After scrub retain acc = 0.978863, forget acc = 0.985714, validation acc = 0.725143\n",
      "Running for checkpoint = 7\n",
      "Initial retain acc = 0.979701, forget acc = 0.985714, validation acc = 0.725714\n",
      "After scrub retain acc = 0.979555, forget acc = 0.983929, validation acc = 0.728571\n",
      "Running for checkpoint = 8\n",
      "Initial retain acc = 0.979701, forget acc = 0.985714, validation acc = 0.725714\n",
      "After scrub retain acc = 0.980138, forget acc = 0.989286, validation acc = 0.728\n",
      "Running for checkpoint = 9\n",
      "Initial retain acc = 0.979701, forget acc = 0.985714, validation acc = 0.725714\n",
      "After scrub retain acc = 0.980321, forget acc = 0.989286, validation acc = 0.728286\n",
      "Total timetaken to run the 10 models is = 67.6 seconds\n"
     ]
    }
   ],
   "source": [
    "T1 = time.time()\n",
    "\n",
    "batch_size = 64\n",
    "retain_loader = DataLoader(retain_dataset, batch_size=batch_size, shuffle=True)\n",
    "forget_loader = DataLoader(forget_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for sd in range(num_checkpoints):\n",
    "    print(f\"Running for checkpoint = {sd}\")\n",
    "    final_model = unlearning(model, model_scrub, retain_loader, forget_loader, validation_loader, device)\n",
    "    # save it as half as there can be space issue.\n",
    "    # https://www.kaggle.com/competitions/neurips-2023-machine-unlearning/discussion/441758\n",
    "    # 32 precision model is 43MB and 16 precision model is 22MB. \n",
    "    state = final_model.half().state_dict()\n",
    "    torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{sd}.pth')\n",
    "\n",
    "T2 = time.time()\n",
    "timetaken_models = round(T2 - T1, 2)\n",
    "print(f\"Total timetaken to run the {num_checkpoints} models is = {timetaken_models} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1207f223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T08:29:47.827055Z",
     "iopub.status.busy": "2023-11-14T08:29:47.826375Z",
     "iopub.status.idle": "2023-11-14T08:29:57.848235Z",
     "shell.execute_reply": "2023-11-14T08:29:57.847182Z"
    },
    "papermill": {
     "duration": 10.717938,
     "end_time": "2023-11-14T08:29:57.850252",
     "exception": false,
     "start_time": "2023-11-14T08:29:47.132314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/tmp/unlearned_checkpoint_0.pth (deflated 7%)\n",
      "  adding: kaggle/tmp/unlearned_checkpoint_1.pth (deflated 7%)\n",
      "  adding: kaggle/tmp/unlearned_checkpoint_2.pth (deflated 7%)\n",
      "  adding: kaggle/tmp/unlearned_checkpoint_3.pth (deflated 7%)\n",
      "  adding: kaggle/tmp/unlearned_checkpoint_4.pth (deflated 7%)\n",
      "  adding: kaggle/tmp/unlearned_checkpoint_5.pth (deflated 7%)\n",
      "  adding: kaggle/tmp/unlearned_checkpoint_6.pth (deflated 7%)\n",
      "  adding: kaggle/tmp/unlearned_checkpoint_7.pth (deflated 7%)\n",
      "  adding: kaggle/tmp/unlearned_checkpoint_8.pth (deflated 7%)\n",
      "  adding: kaggle/tmp/unlearned_checkpoint_9.pth (deflated 7%)\n",
      "Total time taken to zip the 10 models is = 10.02 seconds\n"
     ]
    }
   ],
   "source": [
    "T3 = time.time()\n",
    "# Ensure that submission.zip will contain exactly num_checkpoints \n",
    "# (if this is not the case, an exception will be thrown).\n",
    "unlearned_ckpts = os.listdir('/kaggle/tmp')\n",
    "if len(unlearned_ckpts) != num_checkpoints:\n",
    "    raise RuntimeError(f'Expected exactly {num_checkpoints} checkpoints. The submission will throw an exception otherwise.')\n",
    "\n",
    "subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)\n",
    "T4 = time.time()\n",
    "zip_time_taken = round(T4 - T3, 2)\n",
    "print(f\"Total time taken to zip the {num_checkpoints} models is = {zip_time_taken} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a4a9849",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T08:29:59.228414Z",
     "iopub.status.busy": "2023-11-14T08:29:59.227725Z",
     "iopub.status.idle": "2023-11-14T08:30:00.205696Z",
     "shell.execute_reply": "2023-11-14T08:30:00.204608Z"
    },
    "papermill": {
     "duration": 1.670963,
     "end_time": "2023-11-14T08:30:00.208082",
     "exception": false,
     "start_time": "2023-11-14T08:29:58.537119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199M\tsubmission.zip\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh submission.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012019b1",
   "metadata": {
    "papermill": {
     "duration": 0.692067,
     "end_time": "2023-11-14T08:30:01.652821",
     "exception": false,
     "start_time": "2023-11-14T08:30:00.960754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2892.154047,
   "end_time": "2023-11-14T08:30:05.212636",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-14T07:41:53.058589",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
